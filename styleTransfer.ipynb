{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sys import argv\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNNModel, self).__init__()\n",
    "            self.cnn1 = nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
    "            #self.nl1 = nn.ReLU()\n",
    "            #self.pool1 = nn.AvgPool1d(kernel_size=5)\n",
    "            #self.fc1 = nn.Linear(4096*2500,2**5)\n",
    "            #self.nl3 = nn.ReLU()\n",
    "            #self.fc2 = nn.Linear(2**10,2**5)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = self.cnn1(x)\n",
    "            #out = self.nl1(out)\n",
    "            #out = self.pool1(out)\n",
    "            out = out.view(out.size(0),-1)\n",
    "            #out = self.fc1(out)\n",
    "            #out = self.nl3(out)\n",
    "            #out = self.fc2(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        a, b, c = input.size()  # a=batch size(=1)\n",
    "        # b=number of feature maps\n",
    "        # (c,d)=dimensions of a f. map (N=c*d)\n",
    "        features = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
    "        G = torch.mm(features, features.t())  # compute the gram product\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "        return G.div(a * b * c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self,retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_audio_spectum(filename):\n",
    "        x, fs = librosa.load(filename, duration=58.04) # Duration=58.05 so as to make sizes convenient\n",
    "        S = librosa.stft(x, N_FFT)\n",
    "        p = np.angle(S)\n",
    "        S = np.log1p(np.abs(S))  \n",
    "        return S, fs\n",
    "    \n",
    "style_layers_default = ['conv_1']    \n",
    "style_weight=2500\n",
    "\n",
    "#gives a model with style loss, we backprop over this new model\n",
    "def get_style_model_and_losses(cnn, style_float,style_weight=style_weight, style_layers=style_layers_default): #STYLE WEIGHT\n",
    "        \n",
    "        cnn = copy.deepcopy(cnn)\n",
    "        style_losses = []\n",
    "        model = nn.Sequential()  # the new Sequential module network\n",
    "        gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            gram = gram.cuda()\n",
    "\n",
    "        name = 'conv_1'\n",
    "        model.add_module(name, cnn.cnn1)\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_float).clone()\n",
    "            target_feature_gram = gram(target_feature)\n",
    "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "            model.add_module(\"style_loss_1\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "        #name = 'pool_1'\n",
    "        #model.add_module(name, cnn.pool1)\n",
    "        return model, style_losses\n",
    " 5   \n",
    "def get_input_param_optimizer(input_float):\n",
    "        input_param = nn.Parameter(input_float.data)\n",
    "        #optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
    "        optimizer = optim.Adam([input_param], lr=learning_rate_initial, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "        return input_param, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps= 2500\n",
    "def run_style_transfer(cnn, style_float, input_float, num_steps=num_steps, style_weight=style_weight): #STYLE WEIGHT, NUM_STEPS\n",
    "        print('Building the style transfer model..')\n",
    "        model, style_losses= get_style_model_and_losses(cnn, style_float, style_weight)\n",
    "        input_param, optimizer = get_input_param_optimizer(input_float)\n",
    "        print('Optimizing..')\n",
    "        run = [0]\n",
    "\n",
    "        while run[0] <= num_steps:\n",
    "            def closure():\n",
    "                # correct the values of updated input image\n",
    "                input_param.data.clamp_(0, 1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                model(input_param)\n",
    "                style_score = 0\n",
    "\n",
    "                for sl in style_losses:\n",
    "                    #print('sl is ',sl,' style loss is ',style_score)\n",
    "                    style_score += sl.backward()\n",
    "\n",
    "                run[0] += 1\n",
    "                if run[0] % 100 == 0:\n",
    "                    print(\"run {}:\".format(run))\n",
    "                    print('Style Loss : {:8f}'.format(style_score.data[0])) #CHANGE 4->8 \n",
    "                    print()\n",
    "\n",
    "                return style_score\n",
    "\n",
    "\n",
    "            optimizer.step(closure)\n",
    "        input_param.data.clamp_(0, 1)\n",
    "        return input_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_audio_name = \"inputs//MLK.wav\"\n",
    "style_audio_name = \"inputs/beta.wav\"\n",
    "N_FFT=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
    "content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
    "\n",
    "num_samples_style=style_audio.shape[1]    \n",
    "num_samples_content=content_audio.shape[1]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_audio = style_audio.reshape([1,1025,num_samples_style])\n",
    "content_audio = content_audio.reshape([1,1025,num_samples_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
    "    content_float = Variable((torch.from_numpy(content_audio)).cuda())    \n",
    "else:\n",
    "    style_float = Variable(torch.from_numpy(style_audio))\n",
    "    content_float = Variable(torch.from_numpy(content_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_float = content_float.clone()\n",
    "learning_rate_initial = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 1167])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_float.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 1528])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_float.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [100]:\n",
      "Style Loss : 0.000141\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 0.000058\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 0.000034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = run_style_transfer(cnn, style_float, input_float)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
