{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/parulnith/7f8c174e6ac099e86f0495d3d9a4c01e/untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNnM2w-HCeb1"
   },
   "source": [
    "# Music genre classification notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l3sppZMCydR"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gt3fyg6dCNvX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPe_ebYuDqr5"
   },
   "source": [
    "## Extracting music and features\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use [GTZAN genre collection](http://marsyasweb.appspot.com/download/data_sets/) dataset for classification. \n",
    "<br>\n",
    "<br>\n",
    "The dataset consists of 10 genres i.e\n",
    " * Blues\n",
    " * Classical\n",
    " * Country\n",
    " * Disco\n",
    " * Hiphop\n",
    " * Jazz\n",
    " * Metal\n",
    " * Pop\n",
    " * Reggae\n",
    " * Rock\n",
    " \n",
    "Each genre contains 100 songs. Total dataset: 1000 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neqMS0VoDpN5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfBSVfRCD3PE"
   },
   "source": [
    "## Extracting the Spectrogram for every Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHh3pTEVDdrT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('inferno')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'./dataset/genres/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'./dataset/genres/{g}'):\n",
    "        songname = f'./dataset/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "        plt.axis('off');\n",
    "        plt.savefig(f'dataset/img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
    "        plt.clf()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SszVgjYnFNX9"
   },
   "source": [
    "All the audio files get converted into their respective spectrograms .WE can noe easily extract features from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nw9HpSdFRsW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piwUwgP5Eef9"
   },
   "source": [
    "## Extracting features from Spectrogram\n",
    "\n",
    "\n",
    "We will extract\n",
    "\n",
    "* Mel-frequency cepstral coefficients (MFCC)(20 in number)\n",
    "* Spectral Centroid,\n",
    "* Zero Crossing Rate\n",
    "* Chroma Frequencies\n",
    "* Spectral Roll-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__g8tX8pDeIL"
   },
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBlT448pEqR9"
   },
   "source": [
    "## Writing data to csv file\n",
    "\n",
    "We write the data to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSQmB0PE3Iu"
   },
   "outputs": [],
   "source": [
    "file = open('dataset/data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'./dataset/genres/{g}'):\n",
    "        songname = f'./dataset/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        rmse = librosa.feature.rmse(y=y)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('dataset/data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yfdo1cj6V7d"
   },
   "source": [
    "The data has been extracted into a [data.csv](https://github.com/parulnith/Music-Genre-Classification-with-Python/blob/master/data.csv) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgeCZSKQEp1A"
   },
   "source": [
    "# Analysing the Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Kr5_EdpD9dyh",
    "outputId": "81fd4a29-93fa-44f8-bf90-2f99981f761a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>mfcc3</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00043.au</td>\n",
       "      <td>0.399025</td>\n",
       "      <td>0.127311</td>\n",
       "      <td>2155.654923</td>\n",
       "      <td>2372.403604</td>\n",
       "      <td>5012.019693</td>\n",
       "      <td>0.087165</td>\n",
       "      <td>-109.165355</td>\n",
       "      <td>100.621500</td>\n",
       "      <td>-8.614721</td>\n",
       "      <td>...</td>\n",
       "      <td>6.585774</td>\n",
       "      <td>-8.642621</td>\n",
       "      <td>4.912259</td>\n",
       "      <td>-15.442804</td>\n",
       "      <td>1.538750</td>\n",
       "      <td>-6.732474</td>\n",
       "      <td>1.417774</td>\n",
       "      <td>-3.961750</td>\n",
       "      <td>3.287460</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00012.au</td>\n",
       "      <td>0.269320</td>\n",
       "      <td>0.119072</td>\n",
       "      <td>1361.045467</td>\n",
       "      <td>1567.804596</td>\n",
       "      <td>2739.625101</td>\n",
       "      <td>0.069124</td>\n",
       "      <td>-207.208080</td>\n",
       "      <td>132.799175</td>\n",
       "      <td>-15.438986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613248</td>\n",
       "      <td>0.384877</td>\n",
       "      <td>2.605128</td>\n",
       "      <td>-5.188924</td>\n",
       "      <td>-9.527455</td>\n",
       "      <td>-9.244394</td>\n",
       "      <td>-2.848274</td>\n",
       "      <td>-1.418707</td>\n",
       "      <td>-5.932607</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00026.au</td>\n",
       "      <td>0.278484</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>1198.607665</td>\n",
       "      <td>1573.308974</td>\n",
       "      <td>2478.376680</td>\n",
       "      <td>0.051988</td>\n",
       "      <td>-284.819504</td>\n",
       "      <td>108.785628</td>\n",
       "      <td>9.131956</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.303735</td>\n",
       "      <td>1.601561</td>\n",
       "      <td>2.660517</td>\n",
       "      <td>3.323455</td>\n",
       "      <td>3.258920</td>\n",
       "      <td>-4.551106</td>\n",
       "      <td>0.493845</td>\n",
       "      <td>5.937066</td>\n",
       "      <td>3.231544</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00077.au</td>\n",
       "      <td>0.408876</td>\n",
       "      <td>0.243217</td>\n",
       "      <td>2206.771246</td>\n",
       "      <td>2191.473506</td>\n",
       "      <td>4657.388504</td>\n",
       "      <td>0.111526</td>\n",
       "      <td>-29.010990</td>\n",
       "      <td>104.532914</td>\n",
       "      <td>-30.974207</td>\n",
       "      <td>...</td>\n",
       "      <td>10.786454</td>\n",
       "      <td>-10.558812</td>\n",
       "      <td>6.877709</td>\n",
       "      <td>-10.294858</td>\n",
       "      <td>6.967845</td>\n",
       "      <td>-10.256100</td>\n",
       "      <td>0.705014</td>\n",
       "      <td>-6.000722</td>\n",
       "      <td>1.348955</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00084.au</td>\n",
       "      <td>0.396258</td>\n",
       "      <td>0.235238</td>\n",
       "      <td>2061.150735</td>\n",
       "      <td>2085.159448</td>\n",
       "      <td>4221.149475</td>\n",
       "      <td>0.113397</td>\n",
       "      <td>-38.965941</td>\n",
       "      <td>112.039843</td>\n",
       "      <td>-31.817035</td>\n",
       "      <td>...</td>\n",
       "      <td>13.327049</td>\n",
       "      <td>-10.921602</td>\n",
       "      <td>9.795615</td>\n",
       "      <td>-5.031277</td>\n",
       "      <td>7.200982</td>\n",
       "      <td>-6.754969</td>\n",
       "      <td>2.663612</td>\n",
       "      <td>-4.380430</td>\n",
       "      <td>0.414055</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  chroma_stft      rmse  spectral_centroid  \\\n",
       "0  blues.00043.au     0.399025  0.127311        2155.654923   \n",
       "1  blues.00012.au     0.269320  0.119072        1361.045467   \n",
       "2  blues.00026.au     0.278484  0.076970        1198.607665   \n",
       "3  blues.00077.au     0.408876  0.243217        2206.771246   \n",
       "4  blues.00084.au     0.396258  0.235238        2061.150735   \n",
       "\n",
       "   spectral_bandwidth      rolloff  zero_crossing_rate       mfcc1  \\\n",
       "0         2372.403604  5012.019693            0.087165 -109.165355   \n",
       "1         1567.804596  2739.625101            0.069124 -207.208080   \n",
       "2         1573.308974  2478.376680            0.051988 -284.819504   \n",
       "3         2191.473506  4657.388504            0.111526  -29.010990   \n",
       "4         2085.159448  4221.149475            0.113397  -38.965941   \n",
       "\n",
       "        mfcc2      mfcc3  ...     mfcc12     mfcc13    mfcc14     mfcc15  \\\n",
       "0  100.621500  -8.614721  ...   6.585774  -8.642621  4.912259 -15.442804   \n",
       "1  132.799175 -15.438986  ...  -0.613248   0.384877  2.605128  -5.188924   \n",
       "2  108.785628   9.131956  ...  -3.303735   1.601561  2.660517   3.323455   \n",
       "3  104.532914 -30.974207  ...  10.786454 -10.558812  6.877709 -10.294858   \n",
       "4  112.039843 -31.817035  ...  13.327049 -10.921602  9.795615  -5.031277   \n",
       "\n",
       "     mfcc16     mfcc17    mfcc18    mfcc19    mfcc20  label  \n",
       "0  1.538750  -6.732474  1.417774 -3.961750  3.287460  blues  \n",
       "1 -9.527455  -9.244394 -2.848274 -1.418707 -5.932607  blues  \n",
       "2  3.258920  -4.551106  0.493845  5.937066  3.231544  blues  \n",
       "3  6.967845 -10.256100  0.705014 -6.000722  1.348955  blues  \n",
       "4  7.200982  -6.754969  2.663612 -4.380430  0.414055  blues  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iHrDHCaR9gKR",
    "outputId": "7d32943a-1ad5-4a59-c13a-beebeb36e4c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veD5BgX49hZa"
   },
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyr0aAAsGXjZ"
   },
   "source": [
    "## Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frI5HH4q-1HS"
   },
   "outputs": [],
   "source": [
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Slm8W0-iGVhI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2n8a02zGfvP"
   },
   "source": [
    "## Scaling the Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqcqn-nyAofk"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3VZvbwpGo9R"
   },
   "source": [
    "## Dividing data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1GW3VvQA7Rj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "upuczQ-KBHJ5",
    "outputId": "1431a28b-e8b6-4db2-e505-7e149e37c0d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtoE_FqqBzM8",
    "outputId": "76555a2b-2030-48e1-b52d-d71b4ebae38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ir9XaWgQB0lq",
    "outputId": "2ec90814-19d8-4f27-934a-1ce54406d4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.34212096, -1.40878536, -1.37442596, -1.68976734, -1.44634673,\n",
       "       -0.80893346, -1.56168784,  1.96572022, -0.70660746, -0.49991917,\n",
       "       -0.13900961, -1.03663238, -0.6158062 , -0.94724196,  0.02093777,\n",
       "       -0.20708668, -0.30972376, -0.93535495,  0.11229713, -0.76437544,\n",
       "        0.46671692, -0.1095389 ,  1.19204008, -0.18835474,  0.27617984,\n",
       "       -1.56574893])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vp2yc5FWG04e"
   },
   "source": [
    "# Classification with Keras\n",
    "\n",
    "## Building our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj3sc2uFEUMt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yrsmpI6EjJ2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "bP0hVm4aElS7",
    "outputId": "aacf234d-d0a9-4de4-91be-5fd45a33b279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 0s 373us/step - loss: 2.1270 - acc: 0.2687\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 19us/step - loss: 1.8037 - acc: 0.3500\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 1.6149 - acc: 0.4037\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 1.4459 - acc: 0.4887\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 1.3095 - acc: 0.5363\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 16us/step - loss: 1.1961 - acc: 0.5775\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 1.1084 - acc: 0.6238\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.6537\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 18us/step - loss: 0.9705 - acc: 0.6687\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.9088 - acc: 0.7037\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 16us/step - loss: 0.8602 - acc: 0.7150\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 18us/step - loss: 0.8192 - acc: 0.7300\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 16us/step - loss: 0.7845 - acc: 0.7312\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.7325 - acc: 0.7650\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 16us/step - loss: 0.7077 - acc: 0.7688\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.6820 - acc: 0.7800\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.6523 - acc: 0.7888\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 18us/step - loss: 0.6133 - acc: 0.7825\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.5986 - acc: 0.8025\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 17us/step - loss: 0.5632 - acc: 0.8200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m1J0_wUFK4C",
    "outputId": "ffd3bf36-29ea-437a-987c-9aa600b9dae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 293us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6HrjXeUF0Ko",
    "outputId": "ea282dbd-6f9e-48c7-de2d-dc9afde8949e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc:  0.67\n"
     ]
    }
   ],
   "source": [
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yQmP_f5Kq0w"
   },
   "source": [
    "Tes accuracy is less than training dataa accuracy. This hints at Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-U2qzRJoHV9O"
   },
   "source": [
    "## Validating our approach\n",
    "Let's set apart 200 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJNbvYZoF7ZT"
   },
   "outputs": [],
   "source": [
    "x_val = X_train[:200]\n",
    "partial_x_train = X_train[200:]\n",
    "\n",
    "y_val = y_train[:200]\n",
    "partial_y_train = y_train[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1EkG59EHeEV"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "Dp3G4P3aP4k2",
    "outputId": "25e1a389-1ac2-425b-bd5f-05736b6e9b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "600/600 [==============================] - 0s 583us/step - loss: 2.3202 - acc: 0.1050 - val_loss: 2.1771 - val_acc: 0.2750\n",
      "Epoch 2/30\n",
      "600/600 [==============================] - 0s 22us/step - loss: 2.1286 - acc: 0.3367 - val_loss: 2.0575 - val_acc: 0.3450\n",
      "Epoch 3/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 1.9866 - acc: 0.4067 - val_loss: 1.9308 - val_acc: 0.4150\n",
      "Epoch 4/30\n",
      "600/600 [==============================] - 0s 19us/step - loss: 1.8285 - acc: 0.4567 - val_loss: 1.8155 - val_acc: 0.4100\n",
      "Epoch 5/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 1.6930 - acc: 0.4417 - val_loss: 1.7060 - val_acc: 0.4350\n",
      "Epoch 6/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 1.5637 - acc: 0.4600 - val_loss: 1.6020 - val_acc: 0.4550\n",
      "Epoch 7/30\n",
      "600/600 [==============================] - 0s 19us/step - loss: 1.4436 - acc: 0.5150 - val_loss: 1.5398 - val_acc: 0.4600\n",
      "Epoch 8/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 1.3561 - acc: 0.5767 - val_loss: 1.4859 - val_acc: 0.4600\n",
      "Epoch 9/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 1.2684 - acc: 0.5800 - val_loss: 1.4198 - val_acc: 0.4600\n",
      "Epoch 10/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 1.1840 - acc: 0.6033 - val_loss: 1.3738 - val_acc: 0.4650\n",
      "Epoch 11/30\n",
      "600/600 [==============================] - 0s 19us/step - loss: 1.1301 - acc: 0.6100 - val_loss: 1.3436 - val_acc: 0.4650\n",
      "Epoch 12/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 1.0721 - acc: 0.6233 - val_loss: 1.3707 - val_acc: 0.4950\n",
      "Epoch 13/30\n",
      "600/600 [==============================] - 0s 22us/step - loss: 1.0467 - acc: 0.6300 - val_loss: 1.2934 - val_acc: 0.5100\n",
      "Epoch 14/30\n",
      "600/600 [==============================] - 0s 31us/step - loss: 0.9725 - acc: 0.6567 - val_loss: 1.2578 - val_acc: 0.5100\n",
      "Epoch 15/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.9426 - acc: 0.6817 - val_loss: 1.2444 - val_acc: 0.5100\n",
      "Epoch 16/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.9089 - acc: 0.6933 - val_loss: 1.2295 - val_acc: 0.5250\n",
      "Epoch 17/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.8573 - acc: 0.7250 - val_loss: 1.2686 - val_acc: 0.5150\n",
      "Epoch 18/30\n",
      "600/600 [==============================] - 0s 22us/step - loss: 0.8520 - acc: 0.7267 - val_loss: 1.2596 - val_acc: 0.5250\n",
      "Epoch 19/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.8110 - acc: 0.7317 - val_loss: 1.2276 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "600/600 [==============================] - 0s 22us/step - loss: 0.7699 - acc: 0.7317 - val_loss: 1.2110 - val_acc: 0.5400\n",
      "Epoch 21/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.7426 - acc: 0.7500 - val_loss: 1.2113 - val_acc: 0.5800\n",
      "Epoch 22/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.7220 - acc: 0.7417 - val_loss: 1.1956 - val_acc: 0.5700\n",
      "Epoch 23/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.6908 - acc: 0.7633 - val_loss: 1.2103 - val_acc: 0.5500\n",
      "Epoch 24/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.6673 - acc: 0.7867 - val_loss: 1.1855 - val_acc: 0.5550\n",
      "Epoch 25/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.6243 - acc: 0.8100 - val_loss: 1.2106 - val_acc: 0.5450\n",
      "Epoch 26/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.6426 - acc: 0.7950 - val_loss: 1.1568 - val_acc: 0.5500\n",
      "Epoch 27/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.5823 - acc: 0.8183 - val_loss: 1.1853 - val_acc: 0.5900\n",
      "Epoch 28/30\n",
      "600/600 [==============================] - 0s 20us/step - loss: 0.5640 - acc: 0.8317 - val_loss: 1.1606 - val_acc: 0.5950\n",
      "Epoch 29/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.5418 - acc: 0.8267 - val_loss: 1.1484 - val_acc: 0.5850\n",
      "Epoch 30/30\n",
      "600/600 [==============================] - 0s 21us/step - loss: 0.5134 - acc: 0.8283 - val_loss: 1.1653 - val_acc: 0.5650\n",
      "200/200 [==============================] - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=30,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dljqHfDPI6lH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mvi9it1SI4aR",
    "outputId": "98b01ef2-3935-442b-82d6-45f56e036d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9398117017745972, 0.67]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gudBAhIXJIi2"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xb7bVPSwJQF0",
    "outputId": "aca09c75-1d21-4847-bdd9-a0521dc8d948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llusRQV0JRy9",
    "outputId": "a856289d-883a-47cb-c0fb-ec148330a60a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eoEuSZqJTdU",
    "outputId": "94c17d00-dd7f-40a1-84d2-78d1ebde6103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Pickling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utgt1bXfJVRN"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "#  the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "\n",
    "\n",
    "with open(\"model_num.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_num.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Start Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Load model layers\n",
    "# Model reconstruction from JSON file\n",
    "with open('model_num.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('model_num.h5')\n",
    "\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Variation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(\n",
    "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(\n",
    "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "\n",
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss functions into scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.0)\n",
    "# import ipdb; ipdb.set_trace()\n",
    "layer_features = outputs_dict['dense_5']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                      combination_features)\n",
    "\n",
    "feature_layers = ['dense_6',\n",
    "                  'dense_7', 'dense_8',\n",
    "                  'dense_9']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled9.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
